{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "if IS_COLAB:\n",
        "    !git clone https://github.com/raoulg/ml-21.git\n",
        "    %cd ml-21/5-vision\n",
        "    %pip install loguru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXthnYY5TzaJ"
      },
      "source": [
        "## Baseline model\n",
        "Let's try a simple model, with just one `Conv2D`, one `MaxPool2d`, and one hidden `Dense` layer.\n",
        "\n",
        "With `augment=False` and without `BatchNormalization` or `Dropout` we will start overfitting. Adding those two layers helps stabilizing the overfitting, and setting `augment=True` on the datagenerator helps us squeeze more juice out of our data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-21 13:13:18.611 | INFO     | src.data.make_dataset:get_raw_data:20 - Data not present in ../data/raw, downloading from url\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 39s 0us/step\n",
            "228827136/228813984 [==============================] - 39s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from src.data import make_dataset\n",
        "\n",
        "data_dir = Path(\"../data/raw\")\n",
        "make_dataset.get_raw_data(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-21 13:14:17.280 | INFO     | src.data.make_dataset:create_generators:57 - Creating validation set data generator\n",
            "2021-12-21 13:14:17.362 | INFO     | src.data.make_dataset:create_generators:67 - Creating train set data generator\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 731 images belonging to 5 classes.\n",
            "Found 2939 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "targetsize = (150, 150)\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=0.2)\n",
        "dataflow_kwargs = dict(target_size=targetsize, batch_size=32,\n",
        "                    interpolation=\"bilinear\")\n",
        "\n",
        "train, valid = make_dataset.create_generators(datagen_kwargs, dataflow_kwargs, \n",
        "                                              datadir = data_dir / \"flower_photos\",\n",
        "                                              augment=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.models import base\n",
        "shape = targetsize + (3,)\n",
        "model = base.base_imagemodel(shape, classes = train.num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMDD5LtIHLje"
      },
      "source": [
        "Note how we **don't** use an sigmoid activation in the output layer. We could do that, but this way, the relu can range from $[-Inf, Inf]$ instead of $[0,1]$.. If we omit a sigmoid activation we need to specify that `from_logits=True`. \n",
        "\n",
        "It gives the layers a bit more freedom, while the output is equivalent. If we want to add a sigmoid, we have to set `from_logits=False`.\n",
        "Also note how we don't hardcode the amount of units for the last layer. We obtain the amount of units from the train_generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UWt3HaJaXH4V"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrHSBCA8fK04",
        "outputId": "df3820ea-1982-4872-9618-8bb386846a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 148, 148, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 74, 74, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 175232)            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               22429824  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,431,493\n",
            "Trainable params: 22,431,429\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z3Rd4IYNFYU"
      },
      "source": [
        "So, let's try the model for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "logbase = Path(\"../logs\")\n",
        "logbase.mkdir(parents=True, exist_ok=True)\n",
        "log_dir = logbase / \"basemodel\"\n",
        "check_dir = Path(\"../models/base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGtbM8-hXH__",
        "outputId": "1a5771a8-a3e8-4892-c785-9698de0caf4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "92/92 [==============================] - 44s 469ms/step - loss: 7.6472 - accuracy: 0.3460 - val_loss: 1.5410 - val_accuracy: 0.3461\n",
            "Epoch 3/10\n",
            "92/92 [==============================] - 41s 447ms/step - loss: 1.2775 - accuracy: 0.4491 - val_loss: 1.5141 - val_accuracy: 0.2859\n",
            "Epoch 4/10\n",
            "92/92 [==============================] - 38s 409ms/step - loss: 1.1210 - accuracy: 0.5063 - val_loss: 1.4601 - val_accuracy: 0.3598\n",
            "Epoch 5/10\n",
            "92/92 [==============================] - 36s 394ms/step - loss: 0.9762 - accuracy: 0.5706 - val_loss: 1.4131 - val_accuracy: 0.3461\n",
            "Epoch 6/10\n",
            "92/92 [==============================] - 37s 401ms/step - loss: 0.8659 - accuracy: 0.6410 - val_loss: 1.2977 - val_accuracy: 0.5048\n",
            "Epoch 7/10\n",
            "92/92 [==============================] - 34s 363ms/step - loss: 0.7279 - accuracy: 0.6938 - val_loss: 1.2804 - val_accuracy: 0.5321\n",
            "Epoch 8/10\n",
            "92/92 [==============================] - 35s 376ms/step - loss: 0.6290 - accuracy: 0.7472 - val_loss: 1.2795 - val_accuracy: 0.5417\n",
            "Epoch 9/10\n",
            "92/92 [==============================] - 34s 371ms/step - loss: 0.5394 - accuracy: 0.7795 - val_loss: 1.4144 - val_accuracy: 0.5773\n",
            "Epoch 10/10\n",
            "92/92 [==============================] - 34s 370ms/step - loss: 0.4640 - accuracy: 0.8200 - val_loss: 1.4024 - val_accuracy: 0.5773\n"
          ]
        }
      ],
      "source": [
        "from src.models import train_model\n",
        "\n",
        "epochs = 10\n",
        "history = train_model.train(log_dir, check_dir, \n",
        "                            model=model, \n",
        "                            traingen=train,\n",
        "                            validgen=valid, \n",
        "                            totalepochs=epochs,\n",
        "                            verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBEP3d-c2Oe2"
      },
      "source": [
        "While the model initially is learning, we see it starts overfitting at some point with a train accuracy above 90, while the valid accuracy get's stuck around 50. It's time to set the `augment=True` argument.\n",
        "\n",
        "Pay attention to how I manage to start training from the epoch count where I stopped. So I trained 10 epochs, and will continue with those weights for another additional 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, valid = make_dataset.create_generators(datagen_kwargs, dataflow_kwargs, \n",
        "                                              datadir = data_dir / \"flower_photos\",\n",
        "                                              augment=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15\n",
            "92/92 [==============================] - 42s 453ms/step - loss: 1.3822 - accuracy: 0.4559 - val_loss: 1.5576 - val_accuracy: 0.5157\n",
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15\n",
            "92/92 [==============================] - 45s 485ms/step - loss: 1.2893 - accuracy: 0.4825 - val_loss: 1.5901 - val_accuracy: 0.4856\n",
            "Epoch 13/15\n",
            "92/92 [==============================] - 39s 420ms/step - loss: 1.2627 - accuracy: 0.4961 - val_loss: 1.3708 - val_accuracy: 0.5103\n",
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/15\n",
            "92/92 [==============================] - 39s 421ms/step - loss: 1.2223 - accuracy: 0.5060 - val_loss: 2.4613 - val_accuracy: 0.2763\n",
            "Epoch 15/15\n",
            "92/92 [==============================] - 37s 405ms/step - loss: 1.1791 - accuracy: 0.5158 - val_loss: 1.1114 - val_accuracy: 0.5746\n",
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../models/base/assets\n"
          ]
        }
      ],
      "source": [
        "history = train_model.train(log_dir, check_dir, \n",
        "                            model=model, \n",
        "                            traingen=train,\n",
        "                            validgen=valid, \n",
        "                            totalepochs=epochs+5,\n",
        "                            initial_epochs=epochs\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqypw8l4NSJf"
      },
      "source": [
        "The train accuracy drops (do you understand why?) and model keeps learning  without overfitting. It seems to get stuck at some plateau, but I have run it for over 40 epochs while reaching levels just above 60% of accuracy, which is pretty good for such a basic model with a dataset like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmiFzj2QlJw4"
      },
      "outputs": [],
      "source": [
        "from src.visualization import visualize\n",
        "visualize.evaluate_image_classifier(model, valid, grid=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjqFRd4JN1ly"
      },
      "source": [
        "It is hard to say, why it makes a wrong prediction at some images. Sometimes you can see how it is sidetracked by the background, or some weird details, but sometimes it seems a really obvious example and it is hard to understand, why it doesn't pick it up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPAGAX9EHLjh"
      },
      "source": [
        "\n",
        "We see the model is a slow learner. The loss is fluctiating, but the model keeps learning for quite some time, and doesn't really overfit quickly when the augment is set to True.\n",
        "\n",
        "On the other hand, the image set is pretty hard, with lot's and lot's of different situations, so after all, a simple model like this still does pretty well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir log_dir"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deeplearn3-20201221-2217.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "db087170558c29b52dbaae76c50e592938ff5c7e322e5472320136db97c05a97"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('deep1': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
