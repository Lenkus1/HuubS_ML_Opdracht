{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "if IS_COLAB:\n",
    "    !git clone https://github.com/Lenkus1/HuubS_ML_Opdracht.git\n",
    "    %cd HuubS_ML_Opdracht/template\n",
    "    #%pip install loguru\n",
    "    %pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#%pip install keras-tuner\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#%pip install keras-tuner\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "import random as python_random\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "python_random.seed(42)\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_set = \"/content/HuubS_ML_Opdracht/template/src/data/\"\n",
    "\n",
    "#from HuubS_ML_Opdracht.template.src.data import make_dataset\n",
    "\n",
    "\n",
    "#from HuubS_ML_Opdracht.template.src.visualization import visualize_simple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_set = \"/content/HuubS_ML_Opdracht/template/src/data/\"\n",
    "\n",
    "#from HuubS_ML_Opdracht.template.src.data import make_dataset\n",
    "\n",
    "\n",
    "#from HuubS_ML_Opdracht.template.src.visualization import visualize_simple\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_processed = Path(\"../data/processed\")\n",
    "datadir_model = Path(\"../models\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    datadir_processed = \"/content/HuubS_ML_Opdracht/template/data/processed\"\n",
    "    datadir_model = \"/content/HuubS_ML_Opdracht/template/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def cfm_heatmap(\n",
    "    cfm: np.ndarray,\n",
    "    figsize: tuple = (8, 8),\n",
    "    scale: float = None,\n",
    "    labels: List[str] = None,\n",
    "    vmin: float = None,\n",
    "    vmax: float = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    figsize: tuple, default (8,8)\n",
    "    scale: string. The direction over which the numbers are scaled.\n",
    "        Either None, 'total', 'rowwise' or 'colwise'\n",
    "    \"\"\"\n",
    "\n",
    "    if scale == \"total\":\n",
    "        cfm_norm = cfm / np.sum(cfm)\n",
    "    elif scale == \"rowwise\":\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "    elif scale == \"colwise\":\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=0, keepdims=True)\n",
    "    else:\n",
    "        cfm_norm = cfm\n",
    "    plt.figure(figsize=figsize)\n",
    "    if labels is not None:\n",
    "        plot = sns.heatmap(\n",
    "            cfm_norm,\n",
    "            annot=cfm_norm,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "        )\n",
    "    else:\n",
    "        plot = sns.heatmap(cfm_norm, annot=cfm_norm, vmin=vmin, vmax=vmax)\n",
    "    plot.set(xlabel=\"Predicted\", ylabel=\"Target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12414, 64, 64, 3)\n",
      "(3548, 64, 64, 3)\n",
      "(1774, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for f in sorted(os.listdir(datadir_processed)):\n",
    "    folder = os.path.join(datadir_processed, f)\n",
    "    if os.path.isdir(folder):\n",
    "        #print(f\"{f} is a target class\")\n",
    "        for i in sorted(os.listdir(folder)):\n",
    "            image=tf.keras.preprocessing.image.load_img(folder+'/'+i, \n",
    "            target_size= (64,64))\n",
    "            image=np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(f)\n",
    "\n",
    "data = np.array(data)\n",
    "#labels = np.array(labels)\n",
    "labels = list(labels)\n",
    "#labels_text = labels\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)\n",
    "#labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2,\n",
    "                                                random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125,\n",
    "                                                random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pre-processing\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255\n",
    "X_valid = X_valid.astype('float32') / 255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjtfs\\AppData\\Local\\Temp/ipykernel_420/752153518.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import HyperModel\n"
     ]
    }
   ],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        KS=hp.Choice('num_kernels',values=[3, 5, 7], default=5)\n",
    "        KI='he_uniform'\n",
    "        KR=regularizers.l2(hp.Float('l2', min_value=0.001, max_value=0.005, default=0.002, step=0.001))\n",
    "      \n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.2),\n",
    "        keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        keras.layers.RandomZoom(0.2),\n",
    "        ])\n",
    "\n",
    "        \n",
    "        model = keras.Sequential()\n",
    "\n",
    "        data_augmentation\n",
    "\n",
    "  \n",
    "        model.add(keras.layers.Conv2D(filters=hp.Choice('num_filters_1', values=[32, 64, 96, 128], default=32), kernel_size=KS,kernel_initializer=KI, kernel_regularizer=KR, input_shape=self.input_shape))\n",
    "        model.add(keras.layers.Conv2D(filters=hp.Choice('num_filters_2', values=[32, 64, 96, 128], default=32), kernel_size=KS,kernel_regularizer=KR))\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "       \n",
    "        model.add(keras.layers.Conv2D(filters=hp.Choice('num_filters_3', values=[32, 64, 96, 128], default=64), kernel_size=KS,kernel_regularizer=KR))\n",
    "        model.add(keras.layers.Conv2D(filters=hp.Choice('num_filters_4', values=[32, 64, 96, 128], default=64), kernel_size=KS,kernel_regularizer=KR))\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "\n",
    "       \n",
    "        model.add(keras.layers.Conv2D(filters=hp.Choice('num_filters_5', values=[32, 64, 96, 128], default=64), kernel_size=KS,kernel_regularizer=KR))\n",
    "        model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32, default=128), activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'], default='relu'),kernel_regularizer=KR))\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_3', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=16, default=64), activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'], default='relu'),kernel_regularizer=KR))\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float('dropout_4', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "        model.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "hypermodel = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel,\n",
    "    overwrite=True,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    executions_per_trial=1,\n",
    "    hyperband_iterations=1,\n",
    "    directory=datadir_model,\n",
    "    project_name='galaxies'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 15\n",
      "num_kernels (Choice)\n",
      "{'default': 5, 'conditions': [], 'values': [3, 5, 7], 'ordered': True}\n",
      "l2 (Float)\n",
      "{'default': 0.002, 'conditions': [], 'min_value': 0.001, 'max_value': 0.005, 'step': 0.001, 'sampling': None}\n",
      "num_filters_1 (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 64, 96, 128], 'ordered': True}\n",
      "num_filters_2 (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 64, 96, 128], 'ordered': True}\n",
      "dropout_1 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "num_filters_3 (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [32, 64, 96, 128], 'ordered': True}\n",
      "num_filters_4 (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [32, 64, 96, 128], 'ordered': True}\n",
      "dropout_2 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "num_filters_5 (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [32, 64, 96, 128], 'ordered': True}\n",
      "units_1 (Int)\n",
      "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
      "dense_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "dropout_3 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "units_2 (Int)\n",
      "{'default': 64, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': None}\n",
      "dropout_4 (Float)\n",
      "{'default': 0.25, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.05, 'sampling': None}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 13m 58s]\n",
      "val_accuracy: 0.4763246774673462\n",
      "\n",
      "Best val_accuracy So Far: 0.4763246774673462\n",
      "Total elapsed time: 00h 22m 57s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "num_kernels       |5                 |7                 \n",
      "l2                |0.004             |0.002             \n",
      "num_filters_1     |128               |128               \n",
      "num_filters_2     |64                |32                \n",
      "dropout_1         |0.2               |0.3               \n",
      "num_filters_3     |128               |128               \n",
      "num_filters_4     |128               |64                \n",
      "dropout_2         |0.1               |0.45              \n",
      "num_filters_5     |128               |96                \n",
      "units_1           |64                |192               \n",
      "dense_activation  |sigmoid           |sigmoid           \n",
      "dropout_3         |0.5               |0.15              \n",
      "units_2           |32                |80                \n",
      "dropout_4         |0.15              |0.25              \n",
      "learning_rate     |0.0017261         |0.00019938        \n",
      "tuner/epochs      |3                 |3                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |2                 |2                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Epoch 1/3\n",
      " 95/388 [======>.......................] - ETA: 3:13 - loss: 3.5458 - accuracy: 0.1967"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             validation_data=(X_valid, y_valid),\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the search\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=20,validation_data=(X_valid, y_valid))\n",
    "\n",
    "val_loss_per_epoch = history.history['val_loss']\n",
    "best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(15, 15))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 2) # set the vertical range to [0-2]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = np.argmax(y_test,axis=1)\n",
    "predict = best_model.predict(X_test).argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = LabelEncoder()\n",
    "#y_test = encoder.fit_transform(y_test)\n",
    "#predict = encoder.fit_transform(predict)\n",
    "\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test, predict)\n",
    "cfm_heatmap(cfm, figsize=(12,12), scale='rowwise', vmax= 0.8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fix = keras.models.Sequential([\n",
    "\n",
    "keras.layers.Conv2D(filters=96, kernel_size=(3,3), input_shape=(INPUT_SHAPE)),\n",
    "keras.layers.Conv2D(filters=64, kernel_size=(3,3)),\n",
    "keras.layers.LeakyReLU(alpha=0.2)),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dropout(0.15)\n",
    "\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3)),\n",
    "keras.layers.Conv2D(filters=128, kernel_size=(3,3)),\n",
    "keras.layers.LeakyReLU(alpha=0.2)),\n",
    "keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dropout(0.20)\n",
    "\n",
    "keras.layers.Conv2D(filters=96, kernel_size=(3,3),\n",
    "keras.layers.LeakyReLU(alpha=0.2)),\n",
    "keras.layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(64, activation=\"relu\"),\n",
    "keras.layers.Dropout(0.25),\n",
    "keras.layers.Dense(64, activation=\"relu\"),\n",
    "keras.layers.Dropout(0.5),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "best_model_fix.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        best_model_fix.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "factor=.001, \n",
    "patience=2, \n",
    "min_delta=0.01, \n",
    "mode=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_best_model_fix = best_model_fix.fit(X_train, y_train, epochs=20,\n",
    "validation_data=(X_valid, y_valid), \n",
    "callbacks=[reduceLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist_best_model_fix.history).plot(figsize=(15, 15))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 2) # set the vertical range to [0-2]\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = best_model_fix.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test, predict)\n",
    "cfm_heatmap(cfm, figsize=(12,12), scale='rowwise', vmax= 0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d031ad01bed61ded824456c2d8715b797c0f37e7c3a3e3c3c540f3dac9fb955e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('hu-deeplearning-lSm0EZYV-py3.8': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
